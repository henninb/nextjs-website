---
title: "Setting Up OpenWebUI in Your Local Network with Docker"
date: "2025-09-08"
excerpt: "Complete guide to deploying OpenWebUI using Docker for a self-hosted ChatGPT-like interface with local AI models and privacy-first architecture."
author: "Brian Henning"
tags: ["docker", "openwebui", "ai", "local-network", "self-hosted"]
---

# Setting Up OpenWebUI in Your Local Network with Docker

OpenWebUI has revolutionized the way we interact with AI models by providing a beautiful, ChatGPT-like interface that can be entirely self-hosted. Whether you're looking to maintain privacy, reduce costs, or have complete control over your AI infrastructure, OpenWebUI offers the perfect solution for deploying a local AI chat interface.

## What is OpenWebUI?

OpenWebUI (formerly Ollama WebUI) is an extensible, feature-rich, and user-friendly self-hosted web interface designed to operate entirely offline. It supports various language models including Ollama, OpenAI-compatible APIs, and other AI providers while maintaining complete data privacy.

### Key Features

- **Beautiful Interface**: Modern, responsive design similar to ChatGPT
- **Multi-Model Support**: Works with Ollama, OpenAI, Anthropic, and custom APIs
- **Privacy First**: All data stays on your infrastructure
- **User Management**: Multi-user support with authentication
- **Conversation History**: Persistent chat storage and organization
- **Model Library**: Easy model management and switching
- **Plugin System**: Extensible with custom functions and tools
- **Document Chat**: Upload and chat with documents (RAG)
- **Image Generation**: Support for DALL-E and Stable Diffusion
- **Voice Input/Output**: Speech-to-text and text-to-speech capabilities

## Prerequisites

Before we begin, ensure you have:

- Docker and Docker Compose installed
- At least 8GB RAM (16GB+ recommended for local models)
- 50GB+ free disk space for model storage
- Basic familiarity with Docker networking
- A Linux/macOS environment (Windows with WSL2 works too)

### System Requirements

```bash
# Minimum requirements for OpenWebUI only
CPU: 2 cores
RAM: 2GB
Storage: 10GB

# Recommended for local models (with Ollama)
CPU: 4+ cores
RAM: 16GB+
Storage: 100GB+ SSD
GPU: Optional but recommended for faster inference
```

## Basic Docker Setup

Let's start with a simple OpenWebUI deployment.

### 1. Quick Start with OpenAI API

```bash
# Run OpenWebUI with OpenAI API support
docker run -d \
  --name openwebui \
  --publish 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  --volume openwebui:/app/backend/data \
  --env OPENAI_API_KEY=your_openai_api_key \
  --env WEBUI_SECRET_KEY=your_secret_key \
  --restart unless-stopped \
  ghcr.io/open-webui/open-webui:main
```

### 2. With Local Ollama Backend

```bash
# First, run Ollama
docker run -d \
  --name ollama \
  --publish 11434:11434 \
  --volume ollama:/root/.ollama \
  --gpus=all \
  --restart unless-stopped \
  ollama/ollama

# Download a model
docker exec -it ollama ollama pull llama2

# Run OpenWebUI connected to Ollama
docker run -d \
  --name openwebui \
  --publish 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  --volume openwebui:/app/backend/data \
  --env OLLAMA_BASE_URL=http://ollama:11434 \
  --env WEBUI_SECRET_KEY=your_secret_key \
  --restart unless-stopped \
  --link ollama \
  ghcr.io/open-webui/open-webui:main
```

## Production Docker Compose Setup

For production deployments, Docker Compose provides better management and scalability.

### 1. Complete Stack Configuration

Create `docker-compose.yml`:

```yaml
version: "3.8"

services:
  # OpenWebUI Application
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "3000:8080"
    environment:
      # Basic Configuration
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - WEBUI_NAME=${WEBUI_NAME:-OpenWebUI}
      - DEFAULT_LOCALE=${DEFAULT_LOCALE:-en-US}
      - WEBUI_URL=${WEBUI_URL:-http://localhost:3000}

      # AI Provider APIs
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}

      # Ollama Integration
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_API_BASE_URL=http://ollama:11434/api

      # Database Configuration
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}
      - WEBUI_AUTH=true
      - ENABLE_SIGNUP=${ENABLE_SIGNUP:-true}
      - ENABLE_LOGIN_FORM=true

      # File Upload & Processing
      - ENABLE_RAG_HYBRID_SEARCH=true
      - ENABLE_RAG_WEB_LOADER=true
      - RAG_EMBEDDING_ENGINE=sentence-transformers
      - RAG_EMBEDDING_MODEL=all-MiniLM-L6-v2
      - CHUNK_SIZE=1000
      - CHUNK_OVERLAP=100

      # Image Generation
      - ENABLE_IMAGE_GENERATION=true
      - IMAGE_GENERATION_ENGINE=openai
      - AUTOMATIC1111_BASE_URL=http://stable-diffusion:7860

      # Audio Features
      - AUDIO_STT_ENGINE=openai
      - AUDIO_TTS_ENGINE=openai

      # Security & Performance
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=X-Forwarded-Email
      - CORS_ALLOW_ORIGIN=*
      - SAFE_MODE=false
      - ENABLE_COMMUNITY_SHARING=false

      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    volumes:
      - openwebui_data:/app/backend/data
      - ./uploads:/app/backend/data/uploads
      - ./vector_db:/app/backend/data/vector_db
      - ./config:/app/backend/data/config
    depends_on:
      - postgres
      - redis
      - ollama
    networks:
      - openwebui-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama for Local Models
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    pull_policy: always
    tty: true
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - openwebui-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: openwebui-postgres
    environment:
      - POSTGRES_DB=${DB_NAME}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-local=trust --auth-host=scram-sha-256
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    networks:
      - openwebui-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Redis for Caching
  redis:
    image: redis:7-alpine
    container_name: openwebui-redis
    command: redis-server --appendonly yes --replica-read-only no
    volumes:
      - redis_data:/data
    networks:
      - openwebui-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: openwebui-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./nginx/logs:/var/log/nginx
    depends_on:
      - openwebui
    networks:
      - openwebui-network
    restart: unless-stopped

  # Stable Diffusion (Optional)
  stable-diffusion:
    image: continuumio/miniconda3
    container_name: stable-diffusion
    ports:
      - "7860:7860"
    volumes:
      - ./stable-diffusion:/opt/stable-diffusion
      - stable_diffusion_data:/opt/data
    command: >
      bash -c "
        conda install -y python=3.10 &&
        pip install torch torchvision xformers --index-url https://download.pytorch.org/whl/cu118 &&
        cd /opt/stable-diffusion &&
        git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git . &&
        python launch.py --api --listen --port 7860 --allow-code --share
      "
    networks:
      - openwebui-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  openwebui_data:
  ollama_data:
  postgres_data:
  redis_data:
  stable_diffusion_data:

networks:
  openwebui-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
```

### 2. Environment Configuration

Create `.env` file:

```bash
# Application Settings
WEBUI_SECRET_KEY=your-super-secret-key-here-change-this
WEBUI_NAME=My Local AI Assistant
DEFAULT_LOCALE=en-US
WEBUI_URL=https://ai.yourdomain.local
LOG_LEVEL=INFO

# Authentication
ENABLE_SIGNUP=true
ENABLE_LOGIN_FORM=true
WEBUI_AUTH=true

# AI Provider API Keys
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
GOOGLE_API_KEY=your-google-api-key

# Database Configuration
DB_NAME=openwebui
DB_USER=openwebui
DB_PASSWORD=your-secure-database-password

# Optional: Custom Model Endpoints
CUSTOM_API_BASE_URL=http://your-custom-api:8000/v1
CUSTOM_API_KEY=your-custom-api-key
```

### 3. Advanced Configuration

Create `config/settings.yaml`:

```yaml
# OpenWebUI Advanced Configuration
ui:
  name: "My Local AI Assistant"
  default_locale: "en-US"
  theme: "dark"
  custom_css: |
    .sidebar { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }

auth:
  enabled: true
  signup_enabled: true
  jwt_expiry_hours: 24
  trusted_header_auth: false
  oauth_providers:
    google:
      client_id: "your-google-oauth-client-id"
      client_secret: "your-google-oauth-secret"
      redirect_uri: "https://ai.yourdomain.local/oauth/google/callback"

models:
  default_model: "llama2"
  model_filter_enabled: true
  allowed_models:
    - "gpt-4"
    - "gpt-3.5-turbo"
    - "claude-3-sonnet"
    - "llama2"
    - "mistral"
    - "codellama"

  # Model-specific settings
  model_params:
    "llama2":
      temperature: 0.7
      max_tokens: 2048
      top_p: 0.9
    "gpt-4":
      temperature: 0.8
      max_tokens: 4096

rag:
  enabled: true
  chunk_size: 1500
  chunk_overlap: 200
  embedding_model: "all-MiniLM-L6-v2"
  reranking_model: "ms-marco-MiniLM-L-12-v2"
  search_enabled: true
  web_loader_enabled: true
  pdf_extract_images: true

audio:
  stt:
    enabled: true
    engine: "openai" # or "whisper"
    model: "whisper-1"
  tts:
    enabled: true
    engine: "openai" # or "elevenlabs"
    voice: "alloy"
    speed: 1.0

image:
  generation:
    enabled: true
    engine: "openai" # or "automatic1111"
    size: "1024x1024"
    quality: "standard"
  vision:
    enabled: true
    max_size: "20MB"

functions:
  enabled: true
  # Custom function definitions
  user_functions:
    - name: "get_weather"
      description: "Get current weather for a location"
      parameters:
        type: "object"
        properties:
          location:
            type: "string"
            description: "City name or location"

security:
  content_security_policy:
    enabled: true
    directives:
      default-src: "'self'"
      script-src: "'self' 'unsafe-inline' 'unsafe-eval'"
      style-src: "'self' 'unsafe-inline'"
      img-src: "'self' data: https:"

  rate_limiting:
    enabled: true
    requests_per_minute: 60
    requests_per_hour: 1000

  cors:
    enabled: true
    allowed_origins:
      - "https://ai.yourdomain.local"
      - "http://localhost:3000"
```

## Network Configuration

### 1. Nginx Configuration

Create `nginx/nginx.conf`:

```nginx
events {
    worker_connections 1024;
    multi_accept on;
    use epoll;
}

http {
    # Basic Settings
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 100M;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" $request_time';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    # Rate Limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=1r/s;

    # Upstream servers
    upstream openwebui {
        server openwebui:8080 max_fails=3 fail_timeout=30s;
    }

    upstream ollama {
        server ollama:11434 max_fails=3 fail_timeout=30s;
    }

    # HTTP to HTTPS redirect
    server {
        listen 80;
        server_name ai.yourdomain.local;
        return 301 https://$server_name$request_uri;
    }

    # Main HTTPS server
    server {
        listen 443 ssl http2;
        server_name ai.yourdomain.local;

        # SSL Configuration
        ssl_certificate /etc/nginx/ssl/fullchain.pem;
        ssl_certificate_key /etc/nginx/ssl/privkey.pem;
        ssl_session_timeout 1d;
        ssl_session_cache shared:MozTLS:10m;
        ssl_session_tickets off;

        # Modern configuration
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;

        # HSTS
        add_header Strict-Transport-Security "max-age=63072000" always;

        # Security headers
        add_header X-Frame-Options DENY always;
        add_header X-Content-Type-Options nosniff always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header Referrer-Policy "no-referrer-when-downgrade" always;
        add_header Content-Security-Policy "default-src 'self' http: https: data: blob: 'unsafe-inline'; frame-ancestors 'self';" always;

        # Main application
        location / {
            limit_req zone=api burst=20 nodelay;

            proxy_pass http://openwebui;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Forwarded-Host $host;
            proxy_set_header X-Forwarded-Port $server_port;

            # WebSocket support
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";

            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 300s;
            proxy_buffering off;
        }

        # API routes with higher rate limits
        location /api/ {
            limit_req zone=api burst=50 nodelay;

            proxy_pass http://openwebui;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            proxy_read_timeout 300s;
            proxy_connect_timeout 75s;
            proxy_send_timeout 300s;
        }

        # Login endpoint with strict rate limiting
        location /api/v1/auths/ {
            limit_req zone=login burst=5 nodelay;

            proxy_pass http://openwebui;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # File upload endpoint
        location /api/v1/files/ {
            client_max_body_size 500M;
            client_body_timeout 300s;

            proxy_pass http://openwebui;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            proxy_read_timeout 300s;
            proxy_connect_timeout 75s;
            proxy_send_timeout 300s;
        }

        # Direct Ollama API access (optional)
        location /ollama/ {
            rewrite ^/ollama/(.*) /$1 break;
            proxy_pass http://ollama;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Health check endpoint
        location /health {
            access_log off;
            proxy_pass http://openwebui/health;
        }

        # Static files caching
        location ~* \.(css|js|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
            proxy_pass http://openwebui;
        }
    }
}
```

### 2. SSL Certificate Setup

```bash
# Option 1: Self-signed certificate for local development
mkdir -p nginx/ssl
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout nginx/ssl/privkey.pem \
  -out nginx/ssl/fullchain.pem \
  -subj "/C=US/ST=State/L=City/O=Organization/CN=ai.yourdomain.local"

# Option 2: Let's Encrypt with Certbot (for public domains)
# docker run -it --rm --name certbot \
#   -v "$PWD/nginx/ssl:/etc/letsencrypt" \
#   -v "$PWD/nginx/webroot:/var/www/html" \
#   certbot/certbot certonly --webroot \
#   -w /var/www/html \
#   -d ai.yourdomain.local
```

### 3. Local DNS Configuration

Add to `/etc/hosts`:

```bash
127.0.0.1 ai.yourdomain.local
```

## Model Management

### 1. Ollama Model Installation

```bash
# Install common models
docker exec -it ollama ollama pull llama2
docker exec -it ollama ollama pull mistral
docker exec -it ollama ollama pull codellama
docker exec -it ollama ollama pull llava  # Vision model

# List installed models
docker exec -it ollama ollama list

# Remove models
docker exec -it ollama ollama rm llama2
```

### 2. Custom Model Configuration

Create `config/models.json`:

```json
{
  "models": [
    {
      "id": "custom-gpt-4",
      "name": "Custom GPT-4",
      "provider": "openai",
      "model": "gpt-4",
      "api_base": "https://api.openai.com/v1",
      "api_key_env": "OPENAI_API_KEY",
      "parameters": {
        "temperature": 0.7,
        "max_tokens": 4096,
        "top_p": 1.0
      }
    },
    {
      "id": "local-llama",
      "name": "Local Llama 2",
      "provider": "ollama",
      "model": "llama2:7b",
      "api_base": "http://ollama:11434",
      "parameters": {
        "temperature": 0.8,
        "num_predict": 2048,
        "top_k": 40,
        "top_p": 0.9
      }
    }
  ]
}
```

## Initialization Scripts

### 1. Database Initialization

Create `init-scripts/01-init-db.sql`:

```sql
-- Create extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "vector";

-- Create custom functions
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Grant permissions
GRANT ALL PRIVILEGES ON DATABASE openwebui TO openwebui;
GRANT ALL ON SCHEMA public TO openwebui;
```

### 2. Initial User Setup

Create `init-scripts/02-create-admin.sh`:

```bash
#!/bin/bash
# Wait for OpenWebUI to be ready
sleep 30

# Create admin user
curl -X POST "http://openwebui:8080/api/v1/auths/signup" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Administrator",
    "email": "admin@yourdomain.local",
    "password": "your-secure-admin-password",
    "role": "admin"
  }'
```

## Deployment and Management

### 1. Full Stack Deployment

```bash
# Create necessary directories
mkdir -p nginx/ssl nginx/logs uploads vector_db config init-scripts

# Set permissions
chmod +x init-scripts/*.sh

# Start the stack
docker-compose up -d

# Monitor startup
docker-compose logs -f openwebui

# Check service status
docker-compose ps
```

### 2. Health Monitoring

```bash
# Check all services
docker-compose ps

# Monitor logs
docker-compose logs -f openwebui
docker-compose logs -f ollama
docker-compose logs -f postgres

# Test connectivity
curl -k https://ai.yourdomain.local/health
curl http://localhost:11434/api/tags  # Check Ollama models
```

### 3. Backup and Recovery

Create `backup.sh`:

```bash
#!/bin/bash
BACKUP_DIR="/backup/openwebui-$(date +%Y%m%d-%H%M%S)"
mkdir -p "$BACKUP_DIR"

# Backup database
docker exec openwebui-postgres pg_dump -U openwebui openwebui > "$BACKUP_DIR/database.sql"

# Backup application data
docker run --rm -v openwebui_data:/data -v "$BACKUP_DIR":/backup alpine \
  tar czf /backup/app-data.tar.gz -C /data .

# Backup Ollama models
docker run --rm -v ollama_data:/data -v "$BACKUP_DIR":/backup alpine \
  tar czf /backup/ollama-models.tar.gz -C /data .

echo "Backup completed: $BACKUP_DIR"
```

## Security Considerations

### 1. Environment Hardening

```yaml
# Add to docker-compose.yml
services:
  openwebui:
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
```

### 2. User Management

```python
# Python script to manage users
import requests
import json

API_BASE = "https://ai.yourdomain.local/api/v1"
ADMIN_TOKEN = "your-admin-jwt-token"

def create_user(name, email, password, role="user"):
    response = requests.post(
        f"{API_BASE}/auths/signup",
        headers={"Authorization": f"Bearer {ADMIN_TOKEN}"},
        json={
            "name": name,
            "email": email,
            "password": password,
            "role": role
        }
    )
    return response.json()

def list_users():
    response = requests.get(
        f"{API_BASE}/users",
        headers={"Authorization": f"Bearer {ADMIN_TOKEN}"}
    )
    return response.json()

# Usage
user = create_user("John Doe", "john@company.com", "secure-password")
users = list_users()
```

### 3. Network Security

```yaml
# Restrict container network access
services:
  openwebui:
    networks:
      openwebui-network:
        ipv4_address: 172.20.0.10
    cap_add:
      - NET_ADMIN
    sysctls:
      - net.ipv4.ip_forward=0

networks:
  openwebui-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
    driver_opts:
      com.docker.network.bridge.enable_icc: "false"
```

## Advanced Features

### 1. RAG Document Processing

```bash
# Upload and process documents
curl -X POST "https://ai.yourdomain.local/api/v1/knowledge/upload" \
  -H "Authorization: Bearer your-token" \
  -F "file=@document.pdf" \
  -F "collection=my-documents"

# Query documents
curl -X POST "https://ai.yourdomain.local/api/v1/knowledge/query" \
  -H "Authorization: Bearer your-token" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What does the document say about API integration?",
    "collection": "my-documents",
    "k": 5
  }'
```

### 2. Custom Functions

Create `functions/weather.py`:

```python
import requests
from typing import Dict, Any

def get_weather(location: str) -> Dict[str, Any]:
    """
    Get current weather for a location

    Args:
        location: City name or location

    Returns:
        Weather information dictionary
    """
    api_key = "your-weather-api-key"
    url = f"https://api.openweathermap.org/data/2.5/weather"

    params = {
        "q": location,
        "appid": api_key,
        "units": "metric"
    }

    response = requests.get(url, params=params)
    if response.status_code == 200:
        data = response.json()
        return {
            "location": data["name"],
            "temperature": data["main"]["temp"],
            "description": data["weather"][0]["description"],
            "humidity": data["main"]["humidity"],
            "wind_speed": data["wind"]["speed"]
        }
    else:
        return {"error": "Could not fetch weather data"}

# Function registration
FUNCTIONS = {
    "get_weather": {
        "function": get_weather,
        "description": "Get current weather for a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City name or location"
                }
            },
            "required": ["location"]
        }
    }
}
```

### 3. API Integration Examples

```python
# Python client example
import openai

client = openai.OpenAI(
    api_key="your-openwebui-api-key",
    base_url="https://ai.yourdomain.local/api/v1"
)

# Chat completion
response = client.chat.completions.create(
    model="llama2",
    messages=[
        {"role": "user", "content": "Explain Docker networking"}
    ],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

```javascript
// JavaScript client example
const response = await fetch(
  "https://ai.yourdomain.local/api/v1/chat/completions",
  {
    method: "POST",
    headers: {
      Authorization: "Bearer your-api-key",
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "gpt-4",
      messages: [{ role: "user", content: "Hello from OpenWebUI!" }],
      stream: true,
    }),
  },
);

const reader = response.body.getReader();
while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  const chunk = new TextDecoder().decode(value);
  console.log(chunk);
}
```

## Troubleshooting

### 1. Common Issues

```bash
# OpenWebUI won't start
docker-compose logs openwebui

# Common fixes:
# - Check environment variables
# - Verify database connectivity
# - Ensure sufficient disk space
# - Check port conflicts

# Database connection issues
docker-compose exec postgres pg_isready -U openwebui -d openwebui

# Ollama model issues
docker exec -it ollama ollama list
docker exec -it ollama ollama pull llama2

# GPU not recognized
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
```

### 2. Performance Optimization

```yaml
# Optimize for performance
services:
  openwebui:
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "2.0"
        reservations:
          memory: 2G
          cpus: "1.0"
    environment:
      - WORKERS=4
      - WORKER_CLASS=uvicorn.workers.UvicornWorker

  ollama:
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=2
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

### 3. Monitoring Setup

```yaml
# Add monitoring services
prometheus:
  image: prom/prometheus
  ports:
    - "9090:9090"
  volumes:
    - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
  networks:
    - openwebui-network

grafana:
  image: grafana/grafana
  ports:
    - "3001:3000"
  environment:
    - GF_SECURITY_ADMIN_PASSWORD=admin
  volumes:
    - grafana_data:/var/lib/grafana
    - ./monitoring/grafana:/etc/grafana/provisioning
  networks:
    - openwebui-network
```

## Conclusion

Setting up OpenWebUI in your local network with Docker provides a powerful, private alternative to cloud-based AI services. This comprehensive setup offers:

- **Complete Privacy**: All conversations and data remain on your infrastructure
- **Cost Control**: Use local models or manage API usage centrally
- **Customization**: Full control over interface, models, and functionality
- **Scalability**: Easy horizontal scaling and load balancing
- **Security**: Network isolation and enterprise-grade authentication
- **Flexibility**: Support for multiple AI providers and custom functions

Whether you're running a small personal setup or deploying for an entire organization, this Docker-based architecture provides the foundation for a robust, self-hosted AI assistant platform.

The combination of OpenWebUI's intuitive interface, Ollama's local model serving, and Docker's containerization creates a powerful solution that puts you in complete control of your AI infrastructure while maintaining the user experience quality of modern cloud services.

export default ({ children }) => <div className="blog-post">{children}</div>;

;
